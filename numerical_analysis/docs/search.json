[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Analysis",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p0_preface.html",
    "href": "p0_preface.html",
    "title": "Preface",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p1_error_analysis.html",
    "href": "p1_error_analysis.html",
    "title": "Error Analysis",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p1_representation.html",
    "href": "p1_representation.html",
    "title": "1  Representation of Numbers",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p2_polynomial_interpolation.html",
    "href": "p2_polynomial_interpolation.html",
    "title": "Polynomial Interpolation",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p2_lagrange.html#the-lagrange-interpolation-formula",
    "href": "p2_lagrange.html#the-lagrange-interpolation-formula",
    "title": "2  Lagrange Interpolation",
    "section": "2.1 The Lagrange interpolation formula",
    "text": "2.1 The Lagrange interpolation formula\nLet \\(\\Pi_n\\) denote the space of real or complex polynomials of degree at most \\(n\\). Lagrange interpolation gives an exact answer to the interpolation problem with arbitrary prescribed support points:\n\nTheorem 2.1: Lagrange interpolation formulaLet \\((x_j,f_j)\\), \\(j=0,\\ldots,n\\) be arbitrary support points with \\(x_j\\neq x_k\\) for \\(j\\neq k\\). Then there exists a unique polynomial \\(P\\in\\Pi_n\\) with \\[\nP(x_j) = f_j, ~j=0,\\ldots,n,\n\\] given by \\[\nP(x) = \\sum_j f_j \\prod_{k:k\\neq j} \\frac{x-x_k}{x_j-x_k}.\n\\]\n\n\n\nProofFor uniqueness, let \\(P, Q\\) be two polynomials in \\(\\Pi_n\\) satisfying \\(P(x_j) = Q(x_j) = f_j\\) for \\(j=0,\\ldots,n\\). Then \\(P-Q\\) is a polynomial of degree \\(n\\) with at least \\(n+1\\) roots at \\(x_0,\\ldots,x_n\\). Therefore \\(P-Q = 0\\), which establishes uniqueness.\nFor existence, it suffices to verify that the Lagrange basis polynomials \\[\nL_j(x) = \\prod_{k\\neq j} \\frac{x-x_k}{x_j-x_k}\n\\] are polynomials of degree at most \\(n\\), and that the formula for \\(P(x)\\) satisfies the required conditions. The former claim is obvious upon expanding the product. For the latter claim, we observe that \\[\nL_j(x_i) = \\prod_{k\\neq j} \\frac{x_i-x_k}{x_j-x_k} = \\delta_{ij},\n\\] where \\(\\delta_{ij}\\) denotes the Kronecker delta. Therefore \\[\nP(x_i) = \\sum_j f_j\\delta_{ij} = f_i,\n\\] and the proof is complete.\n\n\nThe utility of Lagrange interpolation breaks down as follows:\nAdvantages:\n\nLagrange interpolation provides a clean theoretical result guaranteeing the existence of a unique interpolating polynomial. It is thus sometimes called upon when interpolation is used as an intermediary step in the theory of other numerical algorithms.\nThe dependence of \\(P(x)\\) on the support ordinates \\(f_j\\) is simple and linear, which makes it convenient for solving multiple interpolation problems with a shared set of support abcissae \\(x_j\\) but varying ordinates.\n\nDisadvantages:\n\nLagrange interpolation is ill-suited for calculations involving large numbers of support points. Evaluating the Lagrange interpolant requires evaluation of the basis polynomials \\(L_j\\), which requires many multiplications.\nIf the support points are changed, the polynomial has to be calculated from scratch. However, rewriting the Lagrange interpolant in barycentric form can reduce the number of required computations when the set of support points is changed by the addition of a new point, as we will describe shortly."
  },
  {
    "objectID": "p2_lagrange.html#a-linear-algebra-perspective",
    "href": "p2_lagrange.html#a-linear-algebra-perspective",
    "title": "2  Lagrange Interpolation",
    "section": "2.2 A linear algebra perspective",
    "text": "2.2 A linear algebra perspective\nPolynomial interpolation is fundamentally a linear algebra problem. Given support points \\((x_j,f_j)\\), solving for \\(P\\in \\Pi_n\\) with \\(P(x_j) = f_j\\) amounts to solving the system \\[\n\\begin{aligned}\n    a_0x_0^0 + a_1x_0^1 + \\cdots + a_nx_0^n &= f_0,\\\\\n    a_0x_1^0 + a_1x_1^1 + \\cdots + a_nx_1^n &= f_1,\\\\\n    &~~~\\vdots\\\\\n    a_0x_n^0 + a_1x_n^1 + \\cdots + a_nx_n^n &= f_n\n\\end{aligned}\n\\] for the coefficients \\(a_0,\\ldots,a_n\\). Converting to matrix form, this can be written as \\[\n\\begin{pmatrix}\n    1 & x_0 & x_0^2 & \\cdots & x_0^n\\\\\n    1 & x_1 & x_1^2 & \\cdots & x_1^n\\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    1 & x_n & x_n^2 & \\cdots & x_n^n\n\\end{pmatrix}\n\\begin{pmatrix}\n    a_0\\\\\n    a_1\\\\\n    \\vdots\\\\\n    a_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    f_0\\\\\n    f_1\\\\\n    \\vdots\\\\\n    f_n\n\\end{pmatrix}.\n\\] The matrix on the LHS is the well-known Vandermonde matrix. The solution of the interpolation problem amounts to inverting this matrix (which also establishes existence and uniqueness, which is conditional on \\(x_j\\neq x_k\\), \\(j\\neq k\\).)\nInverting the Vandermonde matrix is an expensive operation, especially as the number of support points \\(n\\) grows large. Lagrange interpolation avoids this with a change of basis. The Vandermonde matrix arises because we are considering the interpolation problem over the monomial basis \\(\\{1,x,\\ldots,x_n\\}\\) of \\(\\Pi_n\\). By replacing this basis with the Lagrange basis \\(\\{L_0,\\ldots,L_n\\}\\), and using that \\(L_j(x_k) = \\delta_{jk}\\), we can write the system \\(P(x_j) = \\sum_k b_k L_k(x_j) = f_j\\) as \\[\n\\begin{aligned}\nb_0 L_0(x_0) = b_0 &= f_0,\\\\\nb_1 L_1(x_1) = b_1 &= f_1,\\\\\n&~~~\\vdots\\\\\nb_n L_n(x_n) = b_n &= f_n.\n\\end{aligned}\n\\] In matrix form we find that \\(I\\mbf{b} = \\mbf{f}\\), with \\(\\mbf{b} = (b_0,\\ldots,b_n)\\), \\(\\mbf{f} = (f_0,\\ldots,f_n)\\). That is, we have effectively pre-inverted the Vandermonde matrix using our choice of basis. The cost is that our basis polynomials are significantly more complicated than the monomial basis."
  },
  {
    "objectID": "p2_lagrange.html#barycentric-form",
    "href": "p2_lagrange.html#barycentric-form",
    "title": "2  Lagrange Interpolation",
    "section": "2.3 Barycentric form",
    "text": "2.3 Barycentric form\nThe Lagrange basis polynomials have the following convenient property:\n\nProposition 2.2Let \\(x_j\\), \\(j=0,\\ldots,n\\) be fixed and pairwise distinct, and let \\(L_j\\) denote the corresponding Lagrange basis polynomials. Then \\(\\sum_j L_j = 1\\).\n\n\n\nProof\\(Q = \\sum_j L_j\\) satisfies \\(Q(x_k) = \\sum_j \\delta_{jk} = 1\\) for \\(k=0,\\ldots,n\\). Therefore \\(Q\\) is a polynomial of degree \\(n\\) taking the value \\(1\\) on \\(n+1\\) distinct points. Hence \\(Q\\) must be identically \\(1\\).\n\n\nWe now consider the problem of adding a new point \\((x_{n+1},f_{n+1})\\) to the set of support points. To compute the Lagrange interpolant from scratch with the given formula would be an expensive endeavor, as we would need to compute each \\(L_j\\) using the updated set of support points. However, we can reduce the computational burden of the update by rewriting the Lagrange interpolant in barycentric form.\nFirst, we fix the support abcissae \\(x_0,\\ldots,x_n\\). We observe that the Lagrange basis can be written in the following form: \\[\nL_j(x) = \\prod_k (x-x_k) \\cdot \\frac{\\prod_{k:k\\neq j} (x_j-x_k)^{-1}}{x-x_j} = \\ell(x)\\frac{w_j}{x-x_j},\n\\] where \\[\n\\ell(x) = \\prod_j (x-x_j), w_j = \\prod_{k:k\\neq j} (x_j-x_k)^{-1}.\n\\]\n\nDefinition 2.3: Barycentric weightsThe coefficients \\[\nw_j = \\prod_{k:k\\neq j} (x_j-x_k)^{-1}\n\\] are called the barycentric weights.\n\n\nWe can therefore write \\[\n\\sum_j f_j \\prod_{k:k\\neq j} \\frac{x-x_k}{x_j-x_k} = \\ell(x)\\sum_j f_j\\frac{w_j}{x-x_j}.\n\\]\n\nDefinition 2.4: First barycentric formThe expression \\[\nP(x) = \\ell(x)\\sum_j f_j\\frac{w_j}{x-x_j}\n\\] is called the first barycentric form of the Lagrange interpolating polynomial.\n\n\nIf the barycentric weights \\(w_j\\) are pre-computed, then the first barycentric form can be updated to accommodate a new support point \\((x_{n+1},f_{n+1})\\). The update steps go as follows:\n\n\\(\\ell(x)\\) is updated by multiplying a factor of \\((x-x_{n+1})\\).\nFor \\(j=0,\\ldots,n\\), \\(w_j\\) is updated by dividing the factor \\((x_j-x_{n+1})\\).\nThe \\(j=n+1\\) term is appended to the summation.\n\nThis update therefore costs \\(\\mcal{O}(n)\\) operations.\nThe first barycentric form also improves on the number of flops needed to evaluate the Lagrange polynomial. Evaluating \\(L_j(x)\\) from its definition requires \\(\\mcal{O}(n)\\) multiplications, and therefore evaluating the original form of the Lagrange polynomial requires \\(\\mcal{O}(n^2)\\) operations. But assuming that the barycentric weights are pre-computed and stored, evaluating the first barycentric form requires only the multiplication of \\(\\ell(x)\\) and the evaluated summation, which gives \\(\\mcal{O}(n)\\) multiplications.\nA further transformation can be used to reduce the cost of evaluation even further. We write \\[\n1 = \\sum_j L_j(x) = \\ell(x)\\sum_j \\frac{w_j}{x-x_j},\n\\] and divide the first barycentric form through by this expression, which cancels the \\(\\ell\\) factor and yields the following:\n\nDefinition 2.5: Second barycentric formThe expression \\[\nP(x) = \\frac{\\sum_j f_j\\frac{w_j}{x-x_j}}{\\sum_j \\frac{w_j}{x-x_j}}.\n\\] is called the second barycentric form of the Lagrange interpolating polynomial; alternatively, it is called the true barycentric form.\n\n\nOne advantage of the second barycentric form is that it eliminates the evaluation of \\(\\ell(x)\\) altogether. The evaluation of the second barycentric form requires \\(\\mcal{O}(n)\\) multiplications and \\(\\mcal{O}(n)\\) additions. Moreover, the appearance of \\(w_j(x-x_j)^{-1}\\) in both the numerator and denominator helps to avoid the occurrence of catastrophic cancellation when \\(P(x)\\) is evaluated at a value of \\(x\\) close to a node \\(x_j\\).\n\n\n\n\n\n\nWarning: Indeterminate forms\n\n\n\nA practical implementation of Lagrange interpolation using the barycentric formulas should manually specify \\(P(x_j) = f_j\\) to avoid indeterminate forms or divide-by-zero errors."
  },
  {
    "objectID": "p2_lagrange.html#first-barycentric-form",
    "href": "p2_lagrange.html#first-barycentric-form",
    "title": "2  Lagrange Interpolation",
    "section": "2.4 First barycentric form",
    "text": "2.4 First barycentric form\nThe expression \\[\nP(x) = \\ell(x)\\sum_j f_j\\frac{w_j}{x-x_j}\n\\] is called the first barycentric form of the Lagrange interpolating polynomial."
  },
  {
    "objectID": "p2_lagrange.html#second-barycentric-form",
    "href": "p2_lagrange.html#second-barycentric-form",
    "title": "2  Lagrange Interpolation",
    "section": "2.5 Second barycentric form",
    "text": "2.5 Second barycentric form\nThe expression \\[\nP(x) = \\frac{\\sum_j f_j\\frac{w_j}{x-x_j}}{\\sum_j \\frac{w_j}{x-x_j}}.\n\\] is called the second barycentric form of the Lagrange interpolating polynomial; alternatively, it is called the true barycentric form."
  },
  {
    "objectID": "p2_lagrange.html#numerical-examples",
    "href": "p2_lagrange.html#numerical-examples",
    "title": "2  Lagrange Interpolation",
    "section": "2.4 Numerical examples",
    "text": "2.4 Numerical examples\nWe now showcase some numerical examples of Lagrange interpolation in action.\n\n\nCode\nimport numpy as np\nimport numpy.typing as npt\nfrom matplotlib import pyplot as plt\nimport time\nfrom random import random\n\n\n\n\nCode\ndef lagrange_polynomial(X: npt.NDArray[np.float64], F: npt.NDArray[np.float64]):\n    def poly(x):\n        L = np.ones(len(X))\n        for j in range(len(X)):\n            L[j] = np.prod(np.divide(x-X, X[j]-X, out=np.ones(len(X)), where=X!=X[j]))\n        return np.dot(F,L)\n    return poly\n\n\n\n\nCode\nX = np.linspace(-5,5,7)\nF = np.exp(-np.square(X))\nP = lagrange_polynomial(X,F)\n\nT = np.linspace(-5.1,5.1,200)\ny = [P(t) for t in T]\n\nplt.plot(T, np.exp(-np.square(T)), label='exp(-x^2)')\nplt.plot(T, y, label='Interpolated')\nplt.plot(X, F, 'o', label='Support points')\nplt.title('Lagrange interpolation for f(x) = exp(-x^2)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\n\nLagrange interpolation of exp(-x^2) with 7 support points\n\n\n\n\nOur first example implements the original Lagrange interpolation formula. We sample \\(7\\) evenly spaced points from the function \\(e^{-x^2}\\) on the interval \\([-5,5]\\) and plot the degree-\\(6\\) Lagrange interpolant. We see that the Lagrange interpolant performs reasonably well in the interior of the interval, but produces large errors as we move toward the edges of the interval. Outside the interval containing the support points, the error diverges to \\(\\infty\\).\n\n\nCode\nT = np.linspace(-5.1,5.1,200)\nplt.plot(T, np.exp(-np.square(T)), label='exp(-x^2)')\n\nfor N in range(5,15,4):\n    X = np.linspace(-5,5,N)\n    F = np.exp(-np.square(X))\n    P = lagrange_polynomial(X,F)\n    y = [P(t) for t in T]\n    plt.plot(T, y, '--', label='Degree {deg}'.format(deg=N-1))\nplt.title('Approximation accuracy vs. support points')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\n\nLagrange interpolation with increasing number of support points\n\n\n\n\nWe compute multiple Lagrange interpolants to see if increasing the number of support points helps. Unfortunately, we actually observe the opposite: although the approximation improves in the center of the interval, near the edges the approximation worsens as we increase the number of support points.\nThis is, in fact, a problem that is generic to polynomial interpolation, and is known as Runge’s phenomenon. We will explore it in more detail in a following section. For now, we leave the reader with the following warnings regarding the practical application of polynomial interpolation:\n\n\n\n\n\n\nWarning: Extrapolating from polynomial interpolation\n\n\n\nPolynomial interpolation does not extrapolate well outside the interval containing the support points. In general, we expect the error of polynomial interpolation to diverge to \\(\\pm\\infty\\) outside the interval.\n\n\n\n\n\n\n\n\nWarning: Runge’s phenomenon\n\n\n\nIncreasing the number of interpolation points does not necessarily improve the accuracy of polynomial interpolation. In particular, there is a tendency for the approximation error to worsen near the boundaries of the interpolation interval.\n\n\n\n\nCode\ndef barycentric_weights(X: npt.NDArray[np.float64]):\n    W = np.ones(len(X))\n    for j in range(len(X)):\n        W[j] = 1 / np.prod(X[j]-X, where=X!=X[j])\n    return W\n\ndef first_barycentric(X: npt.NDArray[np.float64], F: npt.NDArray[np.float64]):\n    W = barycentric_weights(X)\n    def poly(x):\n        return np.prod(x-X) * np.dot(F, np.divide(W, x-X, out=np.copy(F), where=x!=X))\n    return poly\n\ndef second_barycentric(X: npt.NDArray[np.float64], F: npt.NDArray[np.float64]):\n    W = barycentric_weights(X)\n    def poly(x):\n        return np.dot(F,np.divide(W, x-X, out=np.copy(F), where=x!=X)) / np.sum(np.divide(W, x-X, out=np.copy(F), where=x!=X))\n    return poly\n\n\n\n\nCode\nminPoints = 10\nmaxPoints = 151\nstep = 5\n\nnumTrials = 500\n\nsptPts = np.arange(minPoints, maxPoints, step, dtype=int)\n\nbase_time = np.empty(0, dtype=float)\nfor N in sptPts:\n    X = np.linspace(-5,5,N)\n    F = np.exp(-np.square(X))\n    P = lagrange_polynomial(X,F)\n    st = time.process_time()\n    for i in range(numTrials):\n        P(random())\n    et = time.process_time()\n    base_time = np.append(base_time, et-st)\n\nfirst_bary_time = np.empty(0, dtype=float)\nfor N in sptPts:\n    X = np.linspace(-5,5,N)\n    F = np.exp(-np.square(X))\n    P = first_barycentric(X,F)\n    st = time.process_time()\n    for i in range(numTrials):\n        P(random())\n    et = time.process_time()\n    first_bary_time = np.append(first_bary_time, et-st)\n\nsecond_bary_time = np.empty(0, dtype=float)\nfor N in sptPts:\n    X = np.linspace(-5,5,N)\n    F = np.exp(-np.square(X))\n    P = second_barycentric(X,F)\n    st = time.process_time()\n    for i in range(numTrials):\n        P(random())\n    et = time.process_time()\n    second_bary_time = np.append(second_bary_time, et-st)\n\nplt.plot(sptPts, base_time, 'o', fillstyle='none', label='base')\nplt.plot(sptPts, first_bary_time, '&lt;', fillstyle='none', label='first barycentric form')\nplt.plot(sptPts, second_bary_time, '&gt;', fillstyle='none', label='second barycentric form')\nplt.title('Evaluation time vs. support points')\nplt.xlabel('Support points')\nplt.ylabel('CPU time (s)')\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\n\nPerformance comparison of Lagrange interpolation implementations\n\n\n\n\nLastly, we test the speed of evaluation of the Lagrange polynomial for the three formulas we have provided. We use the three formulas to compute \\(P(x)\\) for evenly spaced support points numbering from \\(10\\) to \\(150\\), and compute the time each formula takes to evaluate \\(P(x)\\) for \\(500\\) uniformly random values of \\(x\\) in \\([0,1]\\). We find that using the original formula for Lagrange interpolation incurs a fairly significant running time penalty as the number of support points grows, while the first and second barycentric formulas manage to maintain consistent running times."
  },
  {
    "objectID": "p2_error_analysis.html",
    "href": "p2_error_analysis.html",
    "title": "3  Error analysis of polynomial interpolation",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p2_error_analysis.html#upper-bounds-on-the-interpolation-error",
    "href": "p2_error_analysis.html#upper-bounds-on-the-interpolation-error",
    "title": "3  Error analysis of polynomial interpolation",
    "section": "3.1 Upper bounds on the interpolation error",
    "text": "3.1 Upper bounds on the interpolation error\nUnder sufficiently strong regularity assumptions, we can provide upper bounds on the approximation error from polynomial interpolation.\n\nTheorem 3.3Assume \\(f\\) is \\(n+1\\) times differentiable. Then for every \\(\\ol{x}\\in\\bb{R}\\), there exists \\(\\xi\\in I[x_0,\\ldots,x_n,\\ol{x}]\\) such that \\[\nf(\\ol{x}) - P_{01\\ldots n}(\\ol{x}) = \\frac{\\omega(\\ol{x})f^{(n+1)}(\\xi)}{(n+1)!}\n\\] where \\[\n\\omega(x) = \\prod_j (x-x_j).\n\\]\n\n\n\nProofFor brevity we write \\(P = P_{01\\ldots n}\\). For \\(\\ol{x} = x_j\\), the claim is trivial since \\(f(x_j) = P(x_j)\\) and \\(\\omega(x_j) = 0\\).\nSo assume \\(\\ol{x}\\neq x_j\\) for any \\(j=0,\\ldots,n\\). Then \\(\\omega(\\ol{x})\\neq 0\\), so there exists a constant \\(K\\) such that \\[\nF(x) = f(x) - P(x) - K\\omega(x)\n\\] vanishes at \\(x = \\ol{x}\\).\nSince \\(F(x_j) = 0\\) for all \\(j\\) by inspection, \\(F\\) has at least \\(n+2\\) zeros in the interval \\(I = I[x_0,\\ldots,x_n,\\ol{x}]\\). By Rolle’s theorem, \\(F'\\) has at least \\(n+1\\) zeros in \\(I\\), \\(F''\\) has at least \\(n\\) zeros, and lastly \\(F^{(n+1)}\\) has at least one zero \\(\\xi\\in I\\). Since \\(P\\) is of degree at most \\(n\\), \\(P^{(n+1)} = 0\\), and thus \\[\nF^{(n+1)}(\\xi) = f^{(n+1)}(\\xi) - K(n+1)! = 0.\n\\] Thus we find that \\[\nK = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}\n\\] for some \\(\\xi\\in I\\). From the definition of \\(K\\) it follows that \\[\nf(\\ol{x}) - P(\\ol{x}) = K\\omega(\\ol{x}),\n\\] and substituting the value of \\(K\\) completes the proof.\n\n\nTherefore the rate of growth of the approximation error with respect to the number of support points is tied to the rate of growth of the function’s derivatives. A function whose higher-order derivaties increase rapidly may exhibit poor approximation accuracy in response to increasing the number of support points."
  }
]