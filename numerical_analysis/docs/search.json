[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Analysis",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p0_preface.html",
    "href": "p0_preface.html",
    "title": "Preface",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p1_error_analysis.html",
    "href": "p1_error_analysis.html",
    "title": "Error Analysis",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p1_representation.html",
    "href": "p1_representation.html",
    "title": "1  Digital representation of numbers",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p2_polynomial_interpolation.html",
    "href": "p2_polynomial_interpolation.html",
    "title": "Polynomial Interpolation",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p2_lagrange.html#the-lagrange-interpolation-formula",
    "href": "p2_lagrange.html#the-lagrange-interpolation-formula",
    "title": "2  Lagrange Interpolation",
    "section": "2.1 The Lagrange interpolation formula",
    "text": "2.1 The Lagrange interpolation formula\nLet \\(\\Pi_n\\) denote the space of real or complex polynomials of degree at most \\(n\\). Lagrange interpolation gives an exact answer to the interpolation problem with arbitrary prescribed support points:\n\nTheorem 2.1: Lagrange interpolation formulaLet \\((x_j,f_j)\\), \\(j=0,\\ldots,n\\) be arbitrary support points with \\(x_j\\neq x_k\\) for \\(j\\neq k\\). Then there exists a unique polynomial \\(P\\in\\Pi_n\\) with \\[\nP(x_j) = f_j, ~j=0,\\ldots,n,\n\\] given by \\[\nP(x) = \\sum_j f_j \\prod_{k:k\\neq j} \\frac{x-x_k}{x_j-x_k}.\n\\]\n\n\n\nProofFor uniqueness, let \\(P, Q\\) be two polynomials in \\(\\Pi_n\\) satisfying \\(P(x_j) = Q(x_j) = f_j\\) for \\(j=0,\\ldots,n\\). Then \\(P-Q\\) is a polynomial of degree \\(n\\) with at least \\(n+1\\) roots at \\(x_0,\\ldots,x_n\\). Therefore \\(P-Q = 0\\), which establishes uniqueness.\nFor existence, it suffices to verify that the Lagrange basis polynomials \\[\nL_j(x) = \\prod_{k\\neq j} \\frac{x-x_k}{x_j-x_k}\n\\] are polynomials of degree at most \\(n\\), and that the formula for \\(P(x)\\) satisfies the required conditions. The former claim is obvious upon expanding the product. For the latter claim, we observe that \\[\nL_j(x_i) = \\prod_{k\\neq j} \\frac{x_i-x_k}{x_j-x_k} = \\delta_{ij},\n\\] where \\(\\delta_{ij}\\) denotes the Kronecker delta. Therefore \\[\nP(x_i) = \\sum_j f_j\\delta_{ij} = f_i,\n\\] and the proof is complete.\n\n\nThe utility of Lagrange interpolation breaks down as follows:\nAdvantages:\n\nLagrange interpolation provides a clean theoretical result guaranteeing the existence of a unique interpolating polynomial. It is thus sometimes called upon when interpolation is used as an intermediary step in the theory of other numerical algorithms.\nThe dependence of \\(P(x)\\) on the support ordinates \\(f_j\\) is simple and linear, which makes it convenient for solving multiple interpolation problems with a shared set of support abcissae \\(x_j\\) but varying ordinates.\n\nDisadvantages:\n\nLagrange interpolation is ill-suited for calculations involving large numbers of support points. Evaluating the Lagrange interpolant requires evaluation of the basis polynomials \\(L_j\\), which requires many multiplications.\nIf the support points are changed, the polynomial has to be calculated from scratch. However, rewriting the Lagrange interpolant in barycentric form can reduce the number of required computations when the set of support points is changed by the addition of a new point, as we will describe shortly."
  },
  {
    "objectID": "p2_lagrange.html#a-linear-algebra-perspective",
    "href": "p2_lagrange.html#a-linear-algebra-perspective",
    "title": "2  Lagrange Interpolation",
    "section": "2.2 A linear algebra perspective",
    "text": "2.2 A linear algebra perspective\nPolynomial interpolation is fundamentally a linear algebra problem. Given support points \\((x_j,f_j)\\), solving for \\(P\\in \\Pi_n\\) with \\(P(x_j) = f_j\\) amounts to solving the system \\[\n\\begin{aligned}\n    a_0x_0^0 + a_1x_0^1 + \\cdots + a_nx_0^n &= f_0,\\\\\n    a_0x_1^0 + a_1x_1^1 + \\cdots + a_nx_1^n &= f_1,\\\\\n    &~~~\\vdots\\\\\n    a_0x_n^0 + a_1x_n^1 + \\cdots + a_nx_n^n &= f_n\n\\end{aligned}\n\\] for the coefficients \\(a_0,\\ldots,a_n\\). Converting to matrix form, this can be written as \\[\n\\begin{pmatrix}\n    1 & x_0 & x_0^2 & \\cdots & x_0^n\\\\\n    1 & x_1 & x_1^2 & \\cdots & x_1^n\\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    1 & x_n & x_n^2 & \\cdots & x_n^n\n\\end{pmatrix}\n\\begin{pmatrix}\n    a_0\\\\\n    a_1\\\\\n    \\vdots\\\\\n    a_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    f_0\\\\\n    f_1\\\\\n    \\vdots\\\\\n    f_n\n\\end{pmatrix}.\n\\] The matrix on the LHS is the well-known Vandermonde matrix. The solution of the interpolation problem amounts to inverting this matrix (which also establishes existence and uniqueness, which is conditional on \\(x_j\\neq x_k\\), \\(j\\neq k\\).)\nInverting the Vandermonde matrix is an expensive operation, especially as the number of support points \\(n\\) grows large. Lagrange interpolation avoids this with a change of basis. The Vandermonde matrix arises because we are considering the interpolation problem over the monomial basis \\(\\{1,x,\\ldots,x_n\\}\\) of \\(\\Pi_n\\). By replacing this basis with the Lagrange basis \\(\\{L_0,\\ldots,L_n\\}\\), and using that \\(L_j(x_k) = \\delta_{jk}\\), we can write the system \\(P(x_j) = \\sum_k b_k L_k(x_j) = f_j\\) as \\[\n\\begin{aligned}\nb_0 L_0(x_0) = b_0 &= f_0,\\\\\nb_1 L_1(x_1) = b_1 &= f_1,\\\\\n&~~~\\vdots\\\\\nb_n L_n(x_n) = b_n &= f_n.\n\\end{aligned}\n\\] In matrix form we find that \\(I\\mbf{b} = \\mbf{f}\\), with \\(\\mbf{b} = (b_0,\\ldots,b_n)\\), \\(\\mbf{f} = (f_0,\\ldots,f_n)\\). That is, we have effectively pre-inverted the Vandermonde matrix using our choice of basis. The cost is that our basis polynomials are significantly more complicated than the monomial basis."
  },
  {
    "objectID": "p2_lagrange.html#barycentric-form",
    "href": "p2_lagrange.html#barycentric-form",
    "title": "2  Lagrange Interpolation",
    "section": "2.3 Barycentric form",
    "text": "2.3 Barycentric form\nThe Lagrange basis polynomials have the following convenient property:\n\nProposition 2.2Let \\(x_j\\), \\(j=0,\\ldots,n\\) be fixed and pairwise distinct, and let \\(L_j\\) denote the corresponding Lagrange basis polynomials. Then \\(\\sum_j L_j = 1\\).\n\n\n\nProof\\(Q = \\sum_j L_j\\) satisfies \\(Q(x_k) = \\sum_j \\delta_{jk} = 1\\) for \\(k=0,\\ldots,n\\). Therefore \\(Q\\) is a polynomial of degree \\(n\\) taking the value \\(1\\) on \\(n+1\\) distinct points. Hence \\(Q\\) must be identically \\(1\\).\n\n\nWe now consider the problem of adding a new point \\((x_{n+1},f_{n+1})\\) to the set of support points. To compute the Lagrange interpolant from scratch with the given formula would be an expensive endeavor, as we would need to compute each \\(L_j\\) using the updated set of support points. However, we can reduce the computational burden of the update by rewriting the Lagrange interpolant in barycentric form.\nFirst, we fix the support abcissae \\(x_0,\\ldots,x_n\\). We observe that the Lagrange basis can be written in the following form: \\[\nL_j(x) = \\prod_k (x-x_k) \\cdot \\frac{\\prod_{k:k\\neq j} (x_j-x_k)^{-1}}{x-x_j} = \\ell(x)\\frac{w_j}{x-x_j},\n\\] where \\[\n\\ell(x) = \\prod_j (x-x_j), w_j = \\prod_{k:k\\neq j} (x_j-x_k)^{-1}.\n\\]\n\nDefinition 2.3: Barycentric weightsThe coefficients \\[\nw_j = \\prod_{k:k\\neq j} (x_j-x_k)^{-1}\n\\] are called the barycentric weights.\n\n\nWe can therefore write \\[\n\\sum_j f_j \\prod_{k:k\\neq j} \\frac{x-x_k}{x_j-x_k} = \\ell(x)\\sum_j f_j\\frac{w_j}{x-x_j}.\n\\]\n\nDefinition 2.4: First barycentric formThe expression \\[\nP(x) = \\ell(x)\\sum_j f_j\\frac{w_j}{x-x_j}\n\\] is called the first barycentric form of the Lagrange interpolating polynomial.\n\n\nIf the barycentric weights \\(w_j\\) are pre-computed, then the first barycentric form can be updated to accommodate a new support point \\((x_{n+1},f_{n+1})\\). The update steps go as follows:\n\n\\(\\ell(x)\\) is updated by multiplying a factor of \\((x-x_{n+1})\\).\nFor \\(j=0,\\ldots,n\\), \\(w_j\\) is updated by dividing the factor \\((x_j-x_{n+1})\\).\nThe \\(j=n+1\\) term is appended to the summation.\n\nThis update therefore costs \\(\\mcal{O}(n)\\) operations.\nThe first barycentric form also improves on the number of flops needed to evaluate the Lagrange polynomial. Evaluating \\(L_j(x)\\) from its definition requires \\(\\mcal{O}(n)\\) multiplications, and therefore evaluating the original form of the Lagrange polynomial requires \\(\\mcal{O}(n^2)\\) operations. But assuming that the barycentric weights are pre-computed and stored, evaluating the first barycentric form requires only the multiplication of \\(\\ell(x)\\) and the evaluated summation, which gives \\(\\mcal{O}(n)\\) multiplications.\nA further transformation can be used to reduce the cost of evaluation even further. We write \\[\n1 = \\sum_j L_j(x) = \\ell(x)\\sum_j \\frac{w_j}{x-x_j},\n\\] and divide the first barycentric form through by this expression, which cancels the \\(\\ell\\) factor and yields the following:\n\nDefinition 2.5: Second barycentric formThe expression \\[\nP(x) = \\frac{\\sum_j f_j\\frac{w_j}{x-x_j}}{\\sum_j \\frac{w_j}{x-x_j}}.\n\\] is called the second barycentric form of the Lagrange interpolating polynomial; alternatively, it is called the true barycentric form.\n\n\nOne advantage of the second barycentric form is that it eliminates the evaluation of \\(\\ell(x)\\) altogether. The evaluation of the second barycentric form requires \\(\\mcal{O}(n)\\) multiplications and \\(\\mcal{O}(n)\\) additions. Moreover, the appearance of \\(w_j(x-x_j)^{-1}\\) in both the numerator and denominator helps to avoid the occurrence of catastrophic cancellation when \\(P(x)\\) is evaluated at a value of \\(x\\) close to a node \\(x_j\\).\n\n\n\n\n\n\nWarning: Indeterminate forms\n\n\n\nA practical implementation of Lagrange interpolation using the barycentric formulas should manually specify \\(P(x_j) = f_j\\) to avoid indeterminate forms or divide-by-zero errors."
  },
  {
    "objectID": "p2_lagrange.html#numerical-examples",
    "href": "p2_lagrange.html#numerical-examples",
    "title": "2  Lagrange Interpolation",
    "section": "2.4 Numerical examples",
    "text": "2.4 Numerical examples\nWe now showcase some numerical examples of Lagrange interpolation in action.\n\n\nCode\nusing InvertedIndices\nusing Plots\nusing LaTeXStrings\nusing Printf\nusing BenchmarkTools\nusing LinearAlgebra\n\n\n\n\nCode\nfunction lagrangeBasis(X::Vector{Float64})\n    function basisPoly(x)::Vector{Float64}\n        L = Vector{Float64}(undef,length(X))\n        for i in eachindex(X)\n            L[i] = prod((x.-X[Not(i)]) ./ (X[i].-X[Not(i)]))\n        end\n        return L\n    end\n    return basisPoly\nend\n\nfunction lagrange(X::Vector{Float64},F::Vector{Float64})\n    function poly(x)::Float64\n        return lagrangeBasis(X)(x) ⋅ F\n    end\n    return poly\nend\n\n\nlagrange (generic function with 1 method)\n\n\n\n\nCode\nT = LinRange(-5.1, 5.1,200)\nplot(T, exp.(-T.^2), label=L\"$\\exp(-x^2)$\", lw=2)\nnPts=9\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nplot!(X, F, seriestype=:scatter, label=\"Support points\")\nplot!(T, lagrange(X,F).(T), label=@sprintf(\"nPts=%d\", nPts), lw=1, ls=:dash)\nplot!(legend=:bottom)\ntitle!(L\"Lagrange interpolation for $f(x) = \\exp(-x^2)$\")\nxlabel!(L\"x\")\nylabel!(L\"y\")\ncurrent()\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur first example implements the original Lagrange interpolation formula. We sample \\(9\\) evenly spaced points from the function \\(e^{-x^2}\\) on the interval \\([-5,5]\\) and plot the degree-\\(6\\) Lagrange interpolant. We see that the Lagrange interpolant performs reasonably well in the interior of the interval, but produces large errors as we move toward the edges of the interval. Outside the interval containing the support points, the error diverges to \\(\\infty\\).\n\n\nCode\nT = LinRange(-5.1, 5.1,200)\nplot(T, exp.(-T.^2), label=L\"$\\exp(-x^2)$\", lw=2)\nfor nPts in 5:4:13\n    local X = collect(LinRange(-5.0, 5.0, nPts))\n    local F = exp.(-X.^2)\n    plot!(T, lagrange(X,F).(T), label=@sprintf(\"nPts=%d\", nPts), lw=1, ls=:dash)\nend\nplot!(legend=:bottom)\ntitle!(\"Approximation performance vs. support points\")\nxlabel!(L\"x\")\nylabel!(L\"y\")\ncurrent()\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe compute multiple Lagrange interpolants to see if increasing the number of support points helps. Unfortunately, we actually observe the opposite: although the approximation improves in the center of the interval, near the edges the approximation worsens as we increase the number of support points.\nThis is, in fact, a problem that is generic to polynomial interpolation, and is known as Runge’s phenomenon. We will explore it in more detail in a following section. For now, we leave the reader with the following warnings regarding the practical application of polynomial interpolation:\n\n\n\n\n\n\nWarning: Extrapolating from polynomial interpolation\n\n\n\nPolynomial interpolation does not extrapolate well outside the interval containing the support points. In general, we expect the error of polynomial interpolation to diverge to \\(\\pm\\infty\\) outside the interval.\n\n\n\n\n\n\n\n\nWarning: Runge’s phenomenon\n\n\n\nIncreasing the number of interpolation points does not necessarily improve the accuracy of polynomial interpolation. In particular, there is a tendency for the approximation error to worsen near the boundaries of the interpolation interval."
  },
  {
    "objectID": "p2_error_analysis.html#upper-bounds-on-the-interpolation-error",
    "href": "p2_error_analysis.html#upper-bounds-on-the-interpolation-error",
    "title": "3  Error analysis of polynomial interpolation",
    "section": "3.1 Upper bounds on the interpolation error",
    "text": "3.1 Upper bounds on the interpolation error\nUnder sufficiently strong regularity assumptions, we can provide upper bounds on the approximation error from polynomial interpolation.\n\nTheorem 3.3Assume \\(f\\) is \\(n+1\\) times differentiable. Then for every \\(\\ol{x}\\in\\bb{R}\\), there exists \\(\\xi\\in I[x_0,\\ldots,x_n,\\ol{x}]\\) such that \\[\nf(\\ol{x}) - P_{01\\ldots n}(\\ol{x}) = \\frac{\\omega(\\ol{x})f^{(n+1)}(\\xi)}{(n+1)!}\n\\] where \\[\n\\omega(x) = \\prod_j (x-x_j).\n\\]\n\n\n\nProofFor brevity we write \\(P = P_{01\\ldots n}\\). For \\(\\ol{x} = x_j\\), the claim is trivial since \\(f(x_j) = P(x_j)\\) and \\(\\omega(x_j) = 0\\).\nSo assume \\(\\ol{x}\\neq x_j\\) for any \\(j=0,\\ldots,n\\). Then \\(\\omega(\\ol{x})\\neq 0\\), so there exists a constant \\(K\\) such that \\[\nF(x) = f(x) - P(x) - K\\omega(x)\n\\] vanishes at \\(x = \\ol{x}\\).\nSince \\(F(x_j) = 0\\) for all \\(j\\) by inspection, \\(F\\) has at least \\(n+2\\) zeros in the interval \\(I = I[x_0,\\ldots,x_n,\\ol{x}]\\). By Rolle’s theorem, \\(F'\\) has at least \\(n+1\\) zeros in \\(I\\), \\(F''\\) has at least \\(n\\) zeros, and lastly \\(F^{(n+1)}\\) has at least one zero \\(\\xi\\in I\\). Since \\(P\\) is of degree at most \\(n\\), \\(P^{(n+1)} = 0\\), and thus \\[\nF^{(n+1)}(\\xi) = f^{(n+1)}(\\xi) - K(n+1)! = 0.\n\\] Thus we find that \\[\nK = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}\n\\] for some \\(\\xi\\in I\\). From the definition of \\(K\\) it follows that \\[\nf(\\ol{x}) - P(\\ol{x}) = K\\omega(\\ol{x}),\n\\] and substituting the value of \\(K\\) completes the proof.\n\n\nTherefore the rate of growth of the approximation error with respect to the number of support points is tied to the rate of growth of the function’s derivatives. A function whose higher-order derivaties increase rapidly may exhibit poor approximation accuracy in response to increasing the number of support points."
  },
  {
    "objectID": "p2_lagrange.html#performance-evaluation",
    "href": "p2_lagrange.html#performance-evaluation",
    "title": "2  Lagrange Interpolation",
    "section": "2.5 Performance evaluation",
    "text": "2.5 Performance evaluation\nLastly, we benchmark the speed of evaluation of the different forms of the Lagrange polynomial, and examine the dependence on the number of support points.\n\n\nCode\nfunction barycentric_weights(X::Vector{Float64})::Vector{Float64}\n    W = Vector{Float64}(undef, length(X))\n    for i in eachindex(X)\n        W[i] = 1.0 / prod(X[i].-X[Not(i)])\n    end\n    return W\nend\n\nfunction first_barycentric(X::Vector{Float64}, F::Vector{Float64})\n    W::Vector{Float64} = barycentric_weights(X)\n    function poly(x)::Float64\n        if x in X\n            return F[findfirst(==(x), X)]\n        end\n        return prod(x.-X) * W ./ (x.-X) ⋅ F\n    end\n    return poly\nend\n\nfunction second_barycentric(X::Vector{Float64}, F::Vector{Float64})\n    W::Vector{Float64} = barycentric_weights(X)\n    function poly(x)::Float64\n        if x in X\n            return F[findfirst(==(x), X)]\n        end\n        return W ./ (x.-X) ⋅ F / sum(W ./ (x.-X))\n    end\n    return poly\nend\n\n\nsecond_barycentric (generic function with 1 method)\n\n\n\n2.5.1 10 support points\n\n\nCode\nnPts = 10::Int\n\nprint(@sprintf(\"Standard Lagrange polynomial, %d support points\", nPts))\n\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nlagrange_standard = lagrange(X,F)\n@benchmark lagrange_standard(randn())\n\n\nStandard Lagrange polynomial, 10 support points\n\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  31.350 μs …  4.091 ms  ┊ GC (min … max): 0.00% … 97.94%\n Time  (median):     32.840 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   37.380 μs ± 85.058 μs  ┊ GC (mean ± σ):  5.83% ±  2.59%\n  ▆▇█▆▄▂▁    ▁▃▃▃▂                                            ▂\n  ████████▇▇▇███████▇▇▇▇▆▇▇██▇██▇▆▆▆▆▅▅▄▅▆▅▅▆▆▆▅▆▅▅▆▇▆▆▆▆▆▇▆▅ █\n  31.4 μs      Histogram: log(frequency) by time      66.7 μs &lt;\n Memory estimate: 35.08 KiB, allocs estimate: 1049.\n\n\n\n\n\nCode\nnPts = 10::Int\n\nprint(@sprintf(\"First barycentric form, %d support points\", nPts))\n\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nfirst_bary = first_barycentric(X,F)\n@benchmark first_bary(randn())\n\n\nFirst barycentric form, 10 support points\n\n\n\nBenchmarkTools.Trial: 10000 samples with 919 evaluations.\n Range (min … max):  113.936 ns …  1.763 μs  ┊ GC (min … max): 0.00% … 91.01%\n Time  (median):     121.058 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   134.598 ns ± 84.204 ns  ┊ GC (mean ± σ):  5.50% ±  8.09%\n  ▇█▆▃▃▂▂▁▁ ▁▁         ▁                                       ▂\n  ████████████████▇▇▆▇██▇▅▆▅▃▆▄▃▆▄▄▅▅▅▄▄▁▄▅▃▁▁▃▁▄▃▁▃▄▁▃▁▁▃▄▃▁▃ █\n  114 ns        Histogram: log(frequency) by time       392 ns &lt;\n Memory estimate: 464 bytes, allocs estimate: 5.\n\n\n\n\n\nCode\nnPts = 10::Int\n\nprint(@sprintf(\"Second barycentric form, %d support points\", nPts))\n\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nsecond_bary = second_barycentric(X,F)\n@benchmark second_bary(randn())\n\n\nSecond barycentric form, 10 support points\n\n\n\nBenchmarkTools.Trial: 10000 samples with 962 evaluations.\n Range (min … max):  86.933 ns …  1.046 μs  ┊ GC (min … max): 0.00% … 84.88%\n Time  (median):     92.124 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   99.198 ns ± 54.447 ns  ┊ GC (mean ± σ):  4.26% ±  6.94%\n   ▄▆█▆▆▅▃                                                    ▁\n  ▆█████████▇███▇▆▆▅▆▆▅▅▅▆▅▆▆▆▆▆▆▅▅▆▄▅▅▅▅▄▅▅▅▂▄▃▄▄▅▆▇▆▅▅▇▇▆▄▄ █\n  86.9 ns      Histogram: log(frequency) by time       165 ns &lt;\n Memory estimate: 320 bytes, allocs estimate: 4.\n\n\n\nBenchmarking on 10 support points, we find that the first and second barycentric forms already outperform the standard form of the Lagrange polynomial significantly on both running time and memory usage. Again, this reflects the fact that evaluating the Lagrange basis polynomials \\(L_j\\) is an expensive operation.\n\n\n2.5.2 100 support points\n\n\nCode\nnPts = 100::Int\n\nprint(@sprintf(\"Standard Lagrange polynomial, %d support points\", nPts))\n\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nlagrange_standard = lagrange(X,F)\n@benchmark lagrange_standard(randn())\n\n\nStandard Lagrange polynomial, 100 support points\n\n\n\nBenchmarkTools.Trial: 1493 samples with 1 evaluation.\n Range (min … max):  3.129 ms …   6.103 ms  ┊ GC (min … max): 0.00% … 43.52%\n Time  (median):     3.163 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   3.348 ms ± 563.095 μs  ┊ GC (mean ± σ):  4.89% ± 10.53%\n  █▆▄▂▁                                               ▁▁▁      \n  █████▇▆▆▅▅▄▅▃▃▅▃▁▁▃▅▅▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆███▇▇▇▅ █\n  3.13 ms      Histogram: log(frequency) by time      5.34 ms &lt;\n Memory estimate: 3.58 MiB, allocs estimate: 109409.\n\n\n\n\n\nCode\nnPts = 100::Int\n\nprint(@sprintf(\"First barycentric form, %d support points\", nPts))\n\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nfirst_bary = first_barycentric(X,F)\n@benchmark first_bary(randn())\n\n\nFirst barycentric form, 100 support points\n\n\n\nBenchmarkTools.Trial: 10000 samples with 442 evaluations.\n Range (min … max):  238.654 ns …   2.080 μs  ┊ GC (min … max): 0.00% … 76.89%\n Time  (median):     279.200 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   308.352 ns ± 159.721 ns  ┊ GC (mean ± σ):  7.72% ± 11.67%\n   ▆█▄▂▁ ▁                                                      ▁\n  █████████▇█▆▆▆▅▆▄▅▁▄▄▃▁▁▁▁▁▁▁▁▁▃▃▄▇▅▁▁▃▁▁▁▁▄▄▁▁▃▁▁▁▁▁▁▄▅▇██▇▇ █\n  239 ns        Histogram: log(frequency) by time       1.23 μs &lt;\n Memory estimate: 2.66 KiB, allocs estimate: 5.\n\n\n\n\n\nCode\nnPts = 100::Int\n\nprint(@sprintf(\"Second barycentric form, %d support points\", nPts))\n\nX = collect(LinRange(-5.0, 5.0, nPts))\nF = exp.(-X.^2)\nsecond_bary = second_barycentric(X,F)\n@benchmark second_bary(randn())\n\n\nSecond barycentric form, 100 support points\n\n\n\nBenchmarkTools.Trial: 10000 samples with 520 evaluations.\n Range (min … max):  217.538 ns …   1.828 μs  ┊ GC (min … max): 0.00% … 79.11%\n Time  (median):     242.696 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   264.421 ns ± 120.654 ns  ┊ GC (mean ± σ):  6.23% ± 10.69%\n  ▁█▇▂▁ ▁▁                                                      ▁\n  ████████▇▇▇▆▅▅▅▄▄▃▄▃▄▁▃▁▁▁▄▅█▄▁▁▁▁▄▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▆▇██ █\n  218 ns        Histogram: log(frequency) by time       1.04 μs &lt;\n Memory estimate: 1.78 KiB, allocs estimate: 4.\n\n\n\nIncreasing the number of support points to 100 shows that the standard form of the Lagrange formula scales poorly, but the barycentric forms remain efficient to evaluate."
  },
  {
    "objectID": "p1_roundoff_error.html#digital-representation-of-numbers",
    "href": "p1_roundoff_error.html#digital-representation-of-numbers",
    "title": "Roundoff error",
    "section": "Digital representation of numbers",
    "text": "Digital representation of numbers\nA fundamental limitation of digital computers is that they must represent real numbers with finite resources. A computer typically uses a fixed finite number \\(n\\) of places to represent a number, which we refer to as the word length. The word length is typically determined at the hardware level. The maximum word length has evolved alongside the proliferation of digital computing. \\(8\\)-bit microprocessors were the first widely used microprocessors in the computer industry, and could store binary representations of numbers with a maximum word length of \\(8\\). In the modern era \\(64\\)-bit processing is widespread, and can represent numbers with a maximum word length of \\(64\\). Methods exist to represent numbers with a higher precision by augmenting the word length. For instance, in a \\(32\\)-bit processor one can use \\(32\\) binary bits to represent a number, yielding something known as single-precision format. Extending this to \\(64\\) bits yields representations of numbers in double-precision format.\nAlthough digital computers represent numbers in base \\(2\\) internally, for the sake of exposition we will represent numbers in base \\(10\\) going forward, except where necessary. Most of the discussion applies equally to binary representations of numbers. There are two types of digital representations of numbers in use. ::: {.Definition #defFixedPointRep}"
  },
  {
    "objectID": "p1_roundoff_error.html#example",
    "href": "p1_roundoff_error.html#example",
    "title": "Roundoff error",
    "section": "Example",
    "text": "Example\nWith \\(8\\) bits, we can (for instance) represent a number with \\(4\\) binary significant digits, e.g. \\(1001.0010_2\\)."
  },
  {
    "objectID": "p1_roundoff_error.html#fixed-point-representation",
    "href": "p1_roundoff_error.html#fixed-point-representation",
    "title": "Roundoff error",
    "section": "Fixed-point representation",
    "text": "Fixed-point representation\nIn fixed-point representation, a number is specified with a fixed number \\(n_1\\) of places preceding the decimal point, and a fixed number \\(n_2\\) of places after the decimal point, with \\(n_1+n_2 = n\\).\n:::\n\n\n\n\n\n\nExample\n\n\n\nThe set of decimal numbers representable in fixed-point representation with \\(n=8\\) place values and \\(n_1=n_2=4\\) is all of the numbers of the form \\(abcd.efgh_10\\).\n\n\n\nDefinition 1: Floating-point representationIn floating-point representation, a number is represented in the form \\(a\\cdot b^c\\), where \\(|a|&lt;1\\), \\(b\\) is the base, and \\(c\\in\\bb{Z}\\). \\(a\\) is referred to as the mantissa, while \\(c\\) is referred to as the \\(exponent\\).\n\n\n\n\n\n\n\n\nSemilogarithmic notation\n\n\n\nSemilogarithmic notation is a notation for floating-point numbers proposed by Rutishauser, taking the form mantissa-base-exponent with the base as a subscript. For example, the number \\(0.23414\\cdot 10^3\\) is expressed as \\(0.23414_{10}3\\).\n\n\nTo express a number in floating-point notation with a finite word length \\(n\\), \\(n_1\\) place values must be devoted to the mantissa and \\(n_2\\) to the exponent with \\(n = n_1 + n_2\\). It is worth noting that the floating-point representation of a number, as defined here, is not unique. For example, \\(3523 = 0.3523_{10}4 = 0.03523_{10}5\\). Uniqueness is recovered by adopting the following convention:\n\nDefinition 2: Normalized floating-point representationA floating-point representation \\(x = a\\cdot b^x\\) is said to be normalized if the first digit of \\(a\\) is nonzero. Equivalently, \\(b^{-1} \\leq a &lt; 1\\). The normalized floating-point representation of a number is unique. If \\(x = a\\cdot b^x\\) is a normalized floating-point representation, then the digits of \\(a\\) (excluding the leading zero) are called significant digits.\n\n\n\nDefinition 3: Machine numbersFor a given base \\(b\\) and word length \\(n\\), the set \\(A \\subset\\bb{R}\\) of real numbers representable exactly in normalized floating point representation is called the set of machine numbers.\n\n\nWhile modern general-purpose digital computers use floating-point numbers for their calculations, some older hardware uses fixed-point representations, and some computational advantages can arise from it. From here onward, we will assume numbers are given in normalized floating-point representation."
  },
  {
    "objectID": "p1_roundoff_error.html",
    "href": "p1_roundoff_error.html",
    "title": "Error analysis",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p1_representation.html#digital-representation-of-numbers",
    "href": "p1_representation.html#digital-representation-of-numbers",
    "title": "1  Representation of numbers",
    "section": "1.1 Digital representation of numbers",
    "text": "1.1 Digital representation of numbers\nA fundamental limitation of digital computers is that they must represent real numbers with finite resources. A computer typically uses a fixed finite number \\(n\\) of places to represent a number, which we refer to as the word length. The word length is typically determined at the hardware level. The maximum word length has evolved alongside the proliferation of digital computing. \\(8\\)-bit microprocessors were the first widely used microprocessors in the computer industry, and could store binary representations of numbers with a maximum word length of \\(8\\). In the modern era \\(64\\)-bit processing is widespread, and can represent numbers with a maximum word length of \\(64\\). Methods exist to represent numbers with a higher precision by augmenting the word length. For instance, in a \\(32\\)-bit processor one can use \\(32\\) binary bits to represent a number, yielding something known as single-precision format. Extending this to \\(64\\) bits yields representations of numbers in double-precision format.\nAlthough digital computers represent numbers in base \\(2\\) internally, for the sake of exposition we will represent numbers in base \\(10\\) going forward, except where necessary. Most of the discussion applies equally to binary representations of numbers. There are two types of digital representations of numbers in use. ::: {.Definition #defFixedPointRep}"
  },
  {
    "objectID": "p1_representation.html#fixed-point-representation",
    "href": "p1_representation.html#fixed-point-representation",
    "title": "1  Digital representation of numbers",
    "section": "1.1 Fixed-point representation",
    "text": "1.1 Fixed-point representation\nIn fixed-point representation, a number is specified with a fixed number \\(n_1\\) of places preceding the decimal point, and a fixed number \\(n_2\\) of places after the decimal point, with \\(n_1+n_2 = n\\).\n:::\n\n\n\n\n\n\nExample\n\n\n\nThe set of decimal numbers representable in fixed-point representation with \\(n=8\\) place values and \\(n_1=n_2=4\\) is all of the numbers of the form \\(abcd.efgh_10\\).\n\n\n\nDefinition 1.1: Floating-point representationIn floating-point representation, a number is represented in the form \\(a\\cdot b^c\\), where \\(|a|&lt;1\\), \\(b\\) is the base, and \\(c\\in\\bb{Z}\\). \\(a\\) is referred to as the mantissa, while \\(c\\) is referred to as the \\(exponent\\).\n\n\n\n\n\n\n\n\nSemilogarithmic notation\n\n\n\nSemilogarithmic notation is a notation for floating-point numbers proposed by Rutishauser, taking the form mantissa-base-exponent with the base as a subscript. For example, the number \\(0.23414\\cdot 10^3\\) is expressed as \\(0.23414_{10}3\\).\n\n\nTo express a number in floating-point notation with a finite word length \\(n\\), \\(n_1\\) place values must be devoted to the mantissa and \\(n_2\\) to the exponent with \\(n = n_1 + n_2\\). It is worth noting that the floating-point representation of a number, as defined here, is not unique. For example, \\(3523 = 0.3523_{10}4 = 0.03523_{10}5\\). Uniqueness is recovered by adopting the following convention:\n\nDefinition 1.2: Normalized floating-point representationA floating-point representation \\(x = a\\cdot b^x\\) is said to be normalized if the first digit of \\(a\\) is nonzero. Equivalently, \\(b^{-1} \\leq a &lt; 1\\). The normalized floating-point representation of a number is unique. If \\(x = a\\cdot b^x\\) is a normalized floating-point representation, then the digits of \\(a\\) (excluding the leading zero) are called significant digits.\n\n\n\nDefinition 1.3: Machine numbersFor a given base \\(b\\) and word length \\(n\\), the set \\(A \\subset\\bb{R}\\) of real numbers representable exactly in normalized floating point representation is called the set of machine numbers.\n\n\nWhile modern general-purpose digital computers use floating-point numbers for their calculations, some older hardware uses fixed-point representations, and some computational advantages can arise from it. From here onward, we will assume numbers are given in normalized floating-point representation."
  },
  {
    "objectID": "p1_fp_arithmetic.html",
    "href": "p1_fp_arithmetic.html",
    "title": "2  Floating-point arithmetic",
    "section": "",
    "text": "\\[\n\\newcommand{\\mcal}[1]{\\mathcal{#1}}\n\\newcommand{\\bb}[1]{\\mathbb{#1}}\n\\newcommand{\\mbf}[1]{\\mathbf{#1}}\n\\newcommand{\\ol}[1]{\\overline{#1}}\n\\newcommand{\\emp}{\\varnothing}\n\\]"
  },
  {
    "objectID": "p1_fp_arithmetic.html#floating-point-conversions",
    "href": "p1_fp_arithmetic.html#floating-point-conversions",
    "title": "2  Fundamentals of floating-point calculations",
    "section": "2.1 Floating-point conversions",
    "text": "2.1 Floating-point conversions\nBecause the set of machine numbers for fixed computing resources is finite, the first task we must contend with when defining mathematics on a digital computer is that of converting real numbers to machine numbers.\nFor what follows, let \\(A\\) denote the set of machine numbers. We set out to define a function \\(\\operatorname{fl}:\\mathbb{R}\\to A\\) that converts a real number \\(x\\) to the machine number that best approximates \\(x\\).\nThere are two strategies in use for converting a real number to a machine-number approximation:\n\nRound-by-chop: This strategy computes the full normalized floating-point representation of \\(x\\), then truncates the mantissa to obtain a machine number.\nRound-to-nearest: This strategy finds the machine number nearest to \\(x\\).\n\n\n\n\n\n\n\nExample\n\n\n\nIf the maximum number of significant digits is \\(4\\), then the number \\(43.146\\) is not a machine number. Round-by-chop produces \\(43.14\\), while round-to-nearest produces \\(43.15\\).\n\n\nMost modern digital computers use round-to-nearest, as it produces smaller roundoff errors on average. Round-by-chop has the advantage of being computationally faster. Round-to-nearest is used in the IEEE standard.\n\n\n\n\n\n\nWarning\n\n\n\nRound-to-nearest rules for certain bases have to impose a tiebreaker rule if there are two equidistant candidates for the closest machine number to \\(x\\). Different tiebreaker rules exist; one example is to always round so that the last stored digit is even. Careful design of tiebreaker rules is warranted because certain tiebreaker rules are biased up or down systematically, and such biases can be accumulated over long calculations."
  },
  {
    "objectID": "p1_fp_arithmetic.html#floating-point-arithmetic",
    "href": "p1_fp_arithmetic.html#floating-point-arithmetic",
    "title": "2  Fundamentals of floating-point calculations",
    "section": "2.3 Floating-point arithmetic",
    "text": "2.3 Floating-point arithmetic"
  },
  {
    "objectID": "p1_fp_arithmetic.html#rounding",
    "href": "p1_fp_arithmetic.html#rounding",
    "title": "2  Fundamentals of floating-point calculations",
    "section": "2.1 Rounding",
    "text": "2.1 Rounding\nBecause the set of machine numbers for fixed computing resources is finite, the first task we must contend with when defining mathematics on a digital computer is that of converting real numbers to machine numbers.\nFor what follows, let \\(A\\) denote the set of machine numbers. We set out to define a function \\(\\operatorname{fl}:\\mathbb{R}\\to A\\) that converts a real number \\(x\\) to the machine number that best approximates \\(x\\).\nThere are two strategies in use for converting a real number \\(x\\) to a machine-number approximation \\(\\operatorname{fl}(x)\\):\n\nRound-to-nearest: \\(x\\) is approximated by the machine number closest to \\(x\\).\nRound-by-chop: \\(x\\) is approximated by the machine number closest to \\(x\\) not exceeding \\(x\\).\n\n\n\n\n\n\n\nExample\n\n\n\nIf the maximum number of significant digits is \\(4\\), then the number \\(43.146\\) is not a machine number. Round-by-chop produces \\(43.14\\), while round-to-nearest produces \\(43.15\\).\n\n\nMost modern digital computers use round-to-nearest, as it produces smaller roundoff errors on average. Round-by-chop has the advantage of being computationally faster. Round-to-nearest is used in the IEEE standard.\n\n\n\n\n\n\nWarning\n\n\n\nRound-to-nearest rules for certain bases have to impose a tiebreaker rule if there are two equidistant candidates for the closest machine number to \\(x\\). Different tiebreaker rules exist; one example is to always round so that the last stored digit is even. Careful design of tiebreaker rules is warranted because certain tiebreaker rules are biased up or down systematically, and such biases can be accumulated over long calculations.\n\n\nWe will assume moving forward that round-to-nearest is our strategy for defining \\(\\operatorname{fl}\\)."
  },
  {
    "objectID": "p1_fp_arithmetic.html#overflow-and-underflow",
    "href": "p1_fp_arithmetic.html#overflow-and-underflow",
    "title": "2  Fundamentals of floating-point calculations",
    "section": "2.2 Overflow and underflow",
    "text": "2.2 Overflow and underflow"
  }
]